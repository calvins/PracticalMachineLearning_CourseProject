---
title: "Predicting the Correct Execution of Weight Lifting Exercises"
author: "Calvin Seto"
date: "January 4, 2016"
output: html_document
---

# Introduction

What a wonderful, modern world we live in today where technology has permeated so many aspects of our lives including health and fitness.  We have many options to choose from to collect data about personal activities.  But, we can do better than just saving and viewing this information.  We can create a prediction model which learns from our data and foresees the outcomes of interesting questions we think of.  This paper will describe how I implemented this process using weight lifting exercise data.  I try to predict how well each subject performed the exercise and which typical mistake was performed out of four mistakes.

Six participants were asked to perform dumbbell lifts five different ways as described in the following table.

Class|Description
-----|-----------
A|exactly according to the specification
B|throwing the elbows to the front
C|lifting the dumbbell only halfway
D|lowering the dumbbell only halfway
E|throwing the hips to the front

Accelerometers were placed on their belt, forearm, arm and dumbbell to collect the activity data.

# The Data Set
## Data Processing
The original data set contains variables that the researchers needed to compute their own features using sliding windows and a feature selection algorithm based on correlation and backtracking.  This resulted in missing values, Excel divide by zero errors, and sparse columns.  I handled these values using R’s read.csv function and the stringsAsFactors and na.strings arguments.

To handle the sparse columns, I created an R function that computes the number of missing values for each variable.  Using the missing value counts, I created new data frames with a subset of columns that do not have any missing values and the rest of the observations in the training and testing sets.

This reduced the number of variables from 160 to 60.  I also discarded 7 of the 60 variables because they were not helpful for prediction since they are the observation number, user name, time stamps, and sliding window data.  The final set of 53 variables comprised of the same 13 variables (roll, pitch, yaw, total acceleration, and accelerometer, gyroscope and magnetometer readings on the x, y, and z axes) collected at the 4 locations (belt, arm, forearm, and dumbbell) and the outcome variable classe.

## Exploratory Data Analysis
Using only the training set, I created summaries of these 52 features to get a quick look at the feature names and range of values.  They all appear to be random real numbers positive, negative and zero.  I created histograms for each and noticed skewed data for magnet_belt_x, y, and z, magnet_arm_z, roll and pitch and accelerometer and gyroscope x, y, and z _dumbbell, magnetometer dumbbell x, y, and z, and gyroscope forearm x,y, and z.
I also used caret’s featurePlot function to see interactions between some groups of features collected at each location.
I created density plots for the 52 variables colored by the classe outcome variable.
I also created a table of the counts of each exercise in the training set and it shows most of the observations fall in class A, according to specification, while the rest are approximately evenly distributed amongst the typical mistake classes B to E.

# Choosing the Prediction Model
As far as I could tell, the exploratory data analysis did not show any imbalance between the outcome and predictors, outliers, or data that is unexplained by predictors.  Transforming the skewed variables is more useful for regression and not classification which is the purpose of this project.  So, I chose to create a model using all 52 features to predict the classe outcome.

## Choosing a classification algorithm
There are many classification algorithms to consider such as linear regression, logistic regression, linear discriminant analysis, classification trees, bagging, random forests, boosting, and support vector machines.

This problem involves five classes and contains an adequate sample size and number of predictors.  Linear regression and logistic regression are fine for two class problems, while early classification algorithms like linear discriminant analysis and classification trees are not as accurate as the ensemble methods of bagging, random forests, and boosting.  Support vector machines may not be as interpretable as the others.  I chose random forests because it reduces the variance in the data by averaging the observations in the data set.  Additionally, it decorrelates the decision trees by randomly selecting predictors used to decide a split in the tree.  Random forests is a state of the art ensemble classification algorithm and has been successful in many Kaggle competitions.

# Training the Prediction Model
It's well known that the accuracy on the training set is optimistic and a better estimate of error comes from an independent testing set.  The cross validation technique helps us calculate the out of sample error.  Cross validation can also help pick variables in the model, choose the type of prediction function, as well as the function's tuning parameters, and compare different predictors.

I used the caret R package to split the training data into training and testing sets with a 75%/25% split.  When training my prediction model, caret's trainControl function helps me choose K-fold cross validation with K = 10 to balance the bias/variance trade-off of the sampled data.

I used the parallel and doParallel packages to take advantage of the 4 cores in the Intel Core i7 processor in my Late 2012 Mac mini desktop with 16GB of memory.  Training time was about 9 minutes.

The random forest algorithm uses the parameter mtry or the number of variables randomly sampled as candidates at each split.  The final model used mtry = 27 with an accuracy of 0.9937492.

```
Random Forest 

14718 samples
52 predictor
5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 13246, 13246, 13247, 13247, 13246, 13246, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9929345  0.9910617  0.002312273  0.002926054
  27    0.9937492  0.9920928  0.001995313  0.002524789
  52    0.9883816  0.9853017  0.002781670  0.003518383

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27.

Random Forest Model Training Summary with 52 predictors

```

Caret includes a variable importance function that estimates the contribution of each variable to the model.  I fit more random forest models by choosing the top five, seven, eight, ten and twenty variables and recorded the training time and accuracy.  Using the top twenty variables in the random forest model, I was able to reduce the training time from 9 minutes to 3 minutes with an accuracy of 0.9912356.

```
Random Forest 

14718 samples
20 predictor
5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 13246, 13246, 13247, 13247, 13246, 13246, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9912356  0.9889135  0.001850337  0.002341485
  11    0.9906243  0.9881409  0.002068700  0.002617140
  20    0.9869549  0.9834990  0.002537371  0.003209113
 
Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2.

Random Forest Model Training Summary with 20 predictors

```


# Expected Out of Sample Error
Caret’s confusionMatrix function produces the expected out of sample error to help us see the performance of my classification model.  First, I made predictions using my random forest model on the 4,904 observations in the testing set.  Then I passed these predictions and the "truth" or the testing set's classe outcome variable to caret's confusionMatrix function producing this output.  I ran confusion matrices for the 52 feature and the 20 feature random forest models.

```
Confusion Matrix and Statistics
 
           Reference
 Prediction    A    B    C    D    E
          A 1392    5    0    0    0
          B    2  942    5    1    0
          C    0    1  846    5    3
          D    0    1    4  798    5
          E    1    0    0    0  893
 
Overall Statistics
                                           
            Accuracy : 0.9933          
              95% CI : (0.9906, 0.9954)
 No Information Rate : 0.2845          
 P-Value [Acc > NIR] : < 2.2e-16       
                                       
               Kappa : 0.9915          
 Mcnemar's Test P-Value : NA              

Statistics by Class:
 
                       Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9978   0.9926   0.9895   0.9925   0.9911
Specificity            0.9986   0.9980   0.9978   0.9976   0.9998
Pos Pred Value         0.9964   0.9916   0.9895   0.9876   0.9989
Neg Pred Value         0.9991   0.9982   0.9978   0.9985   0.9980
Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
Detection Rate         0.2838   0.1921   0.1725   0.1627   0.1821
Detection Prevalence   0.2849   0.1937   0.1743   0.1648   0.1823
Balanced Accuracy      0.9982   0.9953   0.9936   0.9950   0.9954

52 Feature Random Forest Model Confusion Matrix
```

```
 Confusion Matrix and Statistics
 
           Reference
 Prediction    A    B    C    D    E
          A 1390   10    0    0    0
          B    5  930    5    0    0
          C    0    8  846    9    2
          D    0    1    4  795    3
          E    0    0    0    0  896
 
Overall Statistics
                                           
                Accuracy : 0.9904          
                  95% CI : (0.9873, 0.9929)
     No Information Rate : 0.2845          
     P-Value [Acc > NIR] : < 2.2e-16       
                                           
                   Kappa : 0.9879          
  Mcnemar's Test P-Value : NA              
 
Statistics by Class:
 
                       Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9964   0.9800   0.9895   0.9888   0.9945
Specificity            0.9972   0.9975   0.9953   0.9980   1.0000
Pos Pred Value         0.9929   0.9894   0.9780   0.9900   1.0000
Neg Pred Value         0.9986   0.9952   0.9978   0.9978   0.9988
Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
Detection Rate         0.2834   0.1896   0.1725   0.1621   0.1827
Detection Prevalence   0.2855   0.1917   0.1764   0.1637   0.1827
Balanced Accuracy      0.9968   0.9887   0.9924   0.9934   0.9972

20 Feature Random Forest Model Confusion Matrix
```

My 52 feature prediction model has an accuracy of 0.9933 with a 95% Confidence Interval of (0.9906, 0.9954) and P-Value of 2.2e-16.

My 20 feature prediction model has an accuracy of 0.9904 with a 95% Confidence Interval of (0.9873, 0.9929) and P-Value of 2.2e-16.

Using either model, I expect the out of sample error rate to be 99% on a new sample of data.

The output also shows the sensitivity and specificity for each class which helps us measure how well the model correctly predicts the actual exercise type and how well the model does not predict the exercise type of an observation that is a different exercise type.  It tells use the true positive and true negative performance of the model.  
The 52 feature model shows 99% for these values in all classes but class C, while the 20 feature model shows 99% for these values in all classes but classes B, C, and D.

The output shows positive and negative predictive values that reflects the probability that a true positive/true negative is correct given knowledge about the prevalence of classes within the population.
The 52 feature model shows 99% for these values in all classes but classes C and D, while the 20 feature model shows 99% for these values in all classes but classes B and C.

# Conclusion
The 52 feature model correctly classified 1,392 A, 942 B, 846 C, 798 D, and 893 E observations and misclassified 3 A, 7 B, 9 C, 6 D, and 8 E observations.  Of the 4,904 observations in the testing set, it got 4,871 correct and 33 wrong.

The 20 feature model correctly classified 1,390 A, 930 B, 846 C, 795 D, and 896 E observations and misclassified 5 A, 19 B, 9 C, 9 D, and 5 E observations.  Of the 4,904 observations in the testing set, it got 4,857 correct and 47 wrong.

A very good colleague that we all know recommends five attributes of the “best” machine learning method; it is interpretable, simple, accurate, fast, and scalable.

My model is simple and interpretable; it learned from twenty predictors of weight lifting exercise data of five exercises, using a common, effective random forest algorithm to group observations into five classes.

The model is accurate with a greater than 99% accuracy on 4,904 observations in a testing set.

My model is fast, needing about 3 minutes to run on a fairly powerful modern desktop.

The model is scalable, using R code which can easily be migrated to a computer cluster environment composed of commodity servers.

