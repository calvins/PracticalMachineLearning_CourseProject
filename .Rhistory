olive = olive[,-1]
View(olive)
library(caret)
View(olive)
library(caret)
inTrain <- createDataPartition(y=olive$Area, p=0.6, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
modFit <- train(training$Area ~ ., method="rpart", data=training)
modFit <- train(Area ~ ., method="rpart", data=training)
modFit <- train(Area ~ ., method="rpart", data=training)
is.na(olive)
sum(is.na(olive))
modFit <- train(Area ~ ., method="rpart", data=training)
library(rattle)
fancyRpartPlot(modFit$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit,newdata)
options(digits=10)
predict(modFit,newdata)
options(digits=15)
predict(modFit,newdata)
options(digits=3)
predict(modFit,newdata)
round(predict(modFit,newdata),3)
round(predict(modFit,newdata),6)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
set.seed(13234)
modFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl,method="glm",family="binomial",data=trainSA)
View(trainSA)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(caret)
modFit <- train(y~.,data=vowel.train,method="rf",prox=TRUE)
modFit
varImp(modFit,scale=FALSE)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training <- subset(segmentationOriginal, Case=="Train")
testing <- subset(segmentationOriginal, Case=="Test")
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl,method="glm",family="binomial",data=trainSA)
set.seed(13234)
modFit <- train(as.factor(chd) ~ age+alcohol+obesity+tobacco+typea+ldl,method="glm",family="binomial",data=trainSA)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
names(modFit)
modFit$results
modFit$pred
modFit$finalModel
View(trainSA)
missClass(trainSA$chd,predict(modFit$finalModel, trainSA, type="response"))
missClass(testSA$chd,predict(modFit$finalModel, testSA, type="response"))
library(pgmm)
data(olive)
olive = olive[,-1]
library(caret)
inTrain <- createDataPartition(y=olive$Area, p=0.6, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
modFit <- train(Area ~ ., method="rpart", data=training)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit,newdata)
1/3
1/6
colMeans(olive)
View(olive)
signif(1/3,2)
signif(1/3,3)
signif(predict(modFit,newdata),3)
library(pgmm)
data(olive)
olive = olive[,-1]
library(caret)
inTrain <- createDataPartition(y=olive$Area, p=0.6, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
modFit <- train(Area ~ ., method="rpart", data=training)
?plot.enet
31.25-27.09
25*12
200/24
20*24
27.09*24
31.25*24
sqrt(12)*(.01/.04)
?heatmap
?text
?dist
?hclust
?plot
(110-100)-1
(110/100)-1
(90/100)-1
(100/100)-1
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(caret)
modFit1 <- train(y~ .,data=vowel.train,method="rf",prox=TRUE)
modFit2 <- train(y ~ ., method="gbm",data=vowel.train,verbose=FALSE)
predict1 <- predict(modFit1,vowel.test)
predict2 <- predict(modFit2,vowel.test)
confusionMatrix(predict1,vowel.test$y)
confusionMatrix(predict2,vowel.test$y)
predict1
predict2
?intersect
intersect(predict1,predict2)
head(predict1)
head(predict2)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFit1 <- train(diagnosis~ .,data=training,method="rf",prox=TRUE)
modFit2 <- train(diagnosis ~ ., method="gbm",data=training,verbose=FALSE)
modlda = train(diagnosis ~ .,data=training,method="lda")
View(adData)
View(testing)
View(training)
pred1 <- predict(modFit1,testing)
pred2 <- predict(modFit2,testing)
pred3 <- predict(modlda,testing)
predDF <- data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
sqrt(sum((pred1-testing$diagnosis)^2))
sqrt(sum((pred2-testing$diagnosis)^2))
sqrt(sum((pred3-testing$diagnosis)^2))
sqrt(sum((combPred-testing$diagnosis)^2))
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage,
p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
dim(training)
dim(testing)
dim(validation)
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
data=training,
trControl = trainControl(method="cv"),number=3)
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))
View(testing)
str(diagnosis)
str(predictors)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(adData)
str(adData)
View(adData)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFit1 <- train(diagnosis~ .,data=training,method="rf",prox=TRUE)
modFit2 <- train(diagnosis ~ ., method="gbm",data=training,verbose=FALSE)
modlda = train(diagnosis ~ .,data=training,method="lda")
training[,131]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
View(training)
set.seed(233)
library(lars)
x <- as.matrix(training[,1:8])
y <- as.matrix(training[,9])
modFit1 <- lars(x,y, type="lasso")
summary(modFit1)
# select a step with a minimum error
best_step <- modFit1$df[which.min(modFit1$RSS)]
# make predictions
predictions <- predict(modFit1, x, s=best_step, type="fit")$fit
# summarize accuracy
rmse <- mean((y - predictions)^2)
print(rmse)
?plot.enet
library(elasticnet)
?plot.enet
21*26
22*19
546-418
128-35
require(ISLR)
require(tree)
attach(Carseats)
install.packages("tree")
install.packages("tree")
require(ISLR)
require(tree)
attach(Carseats)
hist(Sales)
names(Carseats)
head(Carseats)
High=ifelse(Sales<=8,"No","Yes")
dim(Carseats)
Carseats=data.frame(Carseats, High)
View(Carseats)
tree.carseats=tree(High~.-Sales,data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
set.seed(1011)
train=sample(1:nrow(Carseats),250)
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
plot(tree.carseats);text(tree.carseats,pretty=0)
tree.pred=predict(tree.carseats,Carseats[-train,],type="class")
with(Carseats[-train,],table(tree.pred,High))
(72+33)/150
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats);text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats[-train,],type="class")
with(Carseats[-train,],table(tree.pred,High))
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train=sample(1:nrow(Boston),300)
?Boston
rf.boston=randomForest(medv~.,data=Boston,subset=train)
rf.boston
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]=with(Boston[-train,],mean((medv-pred)^2))
cat(mtry," ")
}
matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type="b",ylab="Mean Squared Error")
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))
head(Boston)
?randomForest
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train=sample(1:nrow(Boston),300)
?Boston
rf.boston=randomForest(medv~.,data=Boston,subset=train)
rf.boston
sd(1,2,3,4,5)
sd(c(1,2,3,4,5))
?sd
33*24
199.72+107.75
199.72+107.75+39.99
199.72+107.75+39.99+7.99
357.50-355.45
125-16
84-12.6
71.4+4.28
84*.15
76/2
88-73.33+15.99-13.33
532.73/2
266.365/4
120+15+13+15+11+15
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
cut2(concrete$CompressiveStrength, c(25,40,50))
install.packages(c("C50", "gbm"))
library(C50)
library(C50)
data(churn)
str(churnTrain)
predictors <- names(churnTrain)[names(churnTrain) != "churn"]
numerics <- c("account_length", "total_day_calls", "total_night_calls") > ##
procValues <- preProcess(churnTrain[,numerics],
method = c("center", "scale", "YeoJohnson"))
library(caret)
procValues <- preProcess(churnTrain[,numerics],
method = c("center", "scale", "YeoJohnson"))
numerics <- c("account_length", "total_day_calls", "total_night_calls")
procValues <- preProcess(churnTrain[,numerics],
method = c("center", "scale", "YeoJohnson"))
trainScaled <- predict(procValues, churnTrain[,numerics])
testScaled  <- predict(procValues, churnTest[,numerics])
library(gbm)
forGBM <- churnTrain
forGBM$churn <- ifelse(forGBM$churn == "yes", 1, 0)
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
grid <- expand.grid(interaction.depth = seq(1, 7, by = 2),
n.trees = seq(100, 1000, by = 50),
shrinkage = c(0.01, 0.1))
set.seed(1)
gbmTune <- train(churn ~ ., data = churnTrain,
method = "gbm",
metric = "ROC",
tuneGrid = grid,
verbose = FALSE,
trControl = ctrl)
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
library(caret)
library(doMC)
install.packages("doMC")
library(doMC)
install.packages("doParallel")
library(doParallel)
28.4+.1+19.2+.1+17.3+.3+16.1+18.3
28.4+.1+19.1+.1+.1+17.3+.2+.1+16.2+18.3
215.52/4
2.5*12
152.50-123
set.seed(123)
seeds <- vector(mode = "list", length = 51)
for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
seeds[[51]] <- sample.int(1000, 1)
ctrl <- trainControl(method = "repeatedcv",
repeats = 5,
seeds = seeds)
set.seed(1)
library(caret)
ctrl <- trainControl(method = "repeatedcv",
repeats = 5,
seeds = seeds)
mod <- train(Species ~ ., data = iris,
method = "knn",
tuneLength = 12,
trControl = ctrl)
ctrl2 <- trainControl(method = "adaptive_cv",
repeats = 5,
verboseIter = TRUE,
seeds = seeds)
set.seed(1)
mod2 <- train(Species ~ ., data = iris,
method = "knn",
tuneLength = 12,
trControl = ctrl2)
library(doParallel); library(caret)
set.seed(123)
seeds <- vector(mode = "list", length = 11)#length is = (n_repeats*nresampling)+1
for(i in 1:10) seeds[[i]]<- sample.int(n=1000, 3) #(3 is the number of tuning parameter, mtry for rf, here equal to ncol(iris)-2)
seeds[[11]]<-sample.int(1000, 1)#for the last model
myControl <- trainControl(method='cv', seeds=seeds, index=createFolds(iris$Species))
cl <- makeCluster(detectCores())
registerDoParallel(cl)
model1 <- train(Species~., iris, method='rf', trControl=myControl)
model2 <- train(Species~., iris, method='rf', trControl=myControl)
stopCluster(cl)
all.equal(predict(model1, type='prob'), predict(model2, type='prob'))
seeds
seeds[[0]]
seeds[[1]]
seeds[[2]]
seeds[[3]]
seeds[[4]]
seeds[[5]]
seeds[[6]]
seeds[[7]]
seeds[[8]]
seeds[[8]]
seeds[[9]]
seeds[[11]]
seeds[[10]]
createFolds(iris$Species)
predict(model1, type='prob')
predict(model2, type='prob')
data(iris)
library(caret)
inTrain = createDataPartition(y=iris$Species, p=0.7, list=F)
train = iris[inTrain,]
test = iris[-inTrain,]
regressors = train[,-5]
species = train[,5]
agg = function(x, type) {
y = count(x)
y = y[order(y$freq, decreasing=T),]
as.character(y$x[1])
}
treebag = bag(regressors, species, B=10,
bagControl = bagControl(fit = train(data=train, species ~ ., method="rpart"),
predict = predict,
aggregate = agg
)
)
agg
treebag = bag(regressors, species, B=10,
bagControl = bagControl(fit = train(data=train, species ~ ., method="rpart"),
predict = predict,
aggregate = agg
)
)
library(caret)
setwd("~/Dropbox/jhudatascience/8_Practical_Machine_Learning/CourseProject")
# setwd("C:/Users/Calvin/Calvinsbiz/Dropbox/jhudatascience/8_Practical_Machine_Learning/CourseProject")
pmlTrain1 <- read.csv("data/pml-training.csv", stringsAsFactors = FALSE, na.strings = c("#DIV/0!","","NA"))
pmlTrain1MissingCounts <- sapply(pmlTrain1, function(x)sum(is.na(x)))
pmlTrain1Complete <- pmlTrain1MissingCounts[pmlTrain1MissingCounts==0]
pmlTrain2 <- pmlTrain1[,names(pmlTrain1Complete)]
inTrain <- createDataPartition(y=pmlTrain2$classe,
p=0.75, list=FALSE)
training <- pmlTrain2[inTrain,]
testing <- pmlTrain2[-inTrain,]
predictors <- training[,8:59]
outcome <- as.factor(training[,60])
# configure parallel
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
# seed
set.seed(168)
# default is bootstrap
fitRFControl <- trainControl(method="cv",
number=10
)
qplot(x=roll_belt, y=yaw_belt,data=training,colour=classe)
qplot(roll_belt,colour=classe,data=training,geom="density")
qplot(x=roll_belt, y=yaw_belt,data=training,colour=classe)
qplot(x=roll_belt, y=pitch_belt,data=training,colour=classe)
qplot(x=pitch_belt, y=yaw_belt,data=training,colour=classe)
qplot(x=roll_belt, y=magnet_dumbbell_z,data=training,colour=classe)
qplot(x=roll_belt, y=pitch_forearm,data=training,colour=classe)
pmlTest0 <- read.csv("data/pml-testing.csv")
pmlTest0MissingCounts <- sapply(pmlTest0, function(x)sum(is.na(x)))
colNamesTest <- pmlTest0MissingCounts[pmlTest0MissingCounts==0]
pmlTest1 <- pmlTest0[,names(colNamesTest)]
1393+941+853+797+897
4881+23
